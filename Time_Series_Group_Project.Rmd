---
title: "Time_Series_Group_Project"
author: "Xavier Bryant"
date: "24/03/2021"
output: pdf_document
---

```{r setup, include=FALSE}
install.packages("forecast")
knitr::opts_chunk$set(echo = TRUE)
library(tsoutliers)
library(knitr)
library(fpp2)
library(SWMPr)
library(forecast)
library(dygraphs)
library(dplyr)
library(TSstudio)
```

# First model 

We use the Boston marathon data from.. 

so original from: https://www.baa.org/races/boston-marathon/results/champions

and inspired by: https://otexts.com/fpp2/nonlinear-regression.html


load the data:


```{r}
marathon <- marathon # the time series is already defined as can be seen below
```


We can see that the data lasts from 1897 to 2016 and is on a yearly basis. we can see an overall decrease in the running times, which makes



```{r}
plot(marathon, main = "Basic Plot of Marathon Data")
```

Now reviewing the stationarity of the plot we see that the plot is not stationary reviewin the acf and the pacf:

```{r}
acf(marathon, lag=150)
pacf(marathon, lag=150)
```

At first look it appears that there is non-stationarity with the slow decrease in the ACF, which then increases again. We also have a some what decaying patten in the PACF, reminiscent of an MA, but there is some odd patterns. There is also several significant peaks. We will try to make our trend weakly stationary for our analysis.

Reffering to our original plot, we will then take the log as we see that variance appears to be  dfferent throughout the series.


```{r}
lmarathon <- log(marathon)
plot(lmarathon)
#maybe think about adding the trend line here. not sure if it's possible.
```




We then can see that there is still in series, downward, that we need to remove, with the first difference to get the log return.



```{r}
dlmarathon <- diff(lmarathon)
plot(dlmarathon)
# if log-transformed is differenced with lag 1, we obtain the so-called log-return
```



There is none constant deadlines. variance appears constant except in the 1910s (?) for the log return series.



```{r}
acf(dlmarathon, lag=150)
pacf(dlmarathon, lag=150)
```


We see there's a decaying patttern in the first few peaks, of the AR, and then two significant peaks in the PACF. difficult to diagnose at the moment. This is tough to tell, although, it's likely we don't have white noise, as we have some significant peaks, we'd like to check if we have a relationship in our data with the Box-Ljung test.


```{r}
Box.test(lmarathon, type="Ljung-Box") 
```

Extremely small p-value so we definitely have a relationship with the past valeus (not white noise?). We can see this through the lag values representation. A definite relationship with past values, with the lags. We can see some almost linear relationships. It does get slightly weaker over time but is still visibly apparent.


```{r}
#lag.plot(lmarathon, lags = 4) #other methods
ts_lags(lmarathon) 
#lagPlot(lmarathon) #other mehods
```





```{r}
# decompose(marathon) #CNAT GET IT TO WORK AT THE MOMENT (NO SEASONAL TREND), ATTEMPT TO SEPERATE TREND FROM RANDOM MOVEMENT, WOULD BE NICE THOUGH
```


We now will try to identify our model. 


```{r}
auto.arima(lmarathon, ic="aic")
```



We then identify our model without outliers below. We first try to find all outliers.



```{r}
tso(lmarathon, types = c("A0", "LS", "TC", "IO", "SLS")) #DON'T TAKE FIRST DIFFERENCE?
```


It defines three IOs which in our context, are difficult to find the logic with as a endogenous change. There is also no Seasonal outliser so we remove those.


```{r}
tso(lmarathon, types = c("A0", "LS", "TC"))
```



We see a few changes. Taking out the outliers we see tht the trend is more smooth. 1921 and 1953 we know that the track changes.


```{r}
outmarathon <- tso(lmarathon, types = c("A0", "LS", "TC")) #considered all the types of outlier possible
plot(outmarathon)
```



```{r}
#CAN'T FIGURE IT OUT: https://anomaly.io/seasonal-trend-decomposition-in-r/index.html
#ts_marathon = ts(marathon, frequency = 1)
#decompose_marathon = decompose(ts_marathon, "multiplicative")
 
#plot(as.ts(decompose_marathon$seasonal))
#plot(as.ts(decompose_marathon$trend))
#plot(as.ts(decompose_marathon$random))
#plot(decompose_marathon)
```

So we reject the null hypothesis which tells as that there is significant correlation meaning that are model is from different from white noise.

ARMA is built on this assumption: conditional mean is a (linear) function of past instances
of the series as well as past innovations.

Any of those weird equations we broke down?

obtain the parameters? Its MA so I think we need to use CSS and then MLE.

In order to be stationary, the unconditional mean and variance of the
MA(q) process should be constant. not sure it is




## Estimating the parameters


```{r}
#PARAMETER ESTIMATION WITH CSS-ML
arima(x = marathon, order=c(0,1,1), method = "CSS-ML")
arima(x = lmarathon, order=c(0,1,1), method = "CSS-ML")
arima(x = diff(lmarathon), order=c(0,0,1), method = "CSS-ML") #honestly doesn't really change that much

#down bias of estimate important to mention
```

so the -0.6492 is the theta. 





```{r}
#could be a cool graph
library(ggfortify)
library(magrittr) # for piping %>%
library(dplyr)
library(changepoint)
# Plot ts objects
autoplot(marathon)
# Identify change points in mean and variance
marathon %>%
  changepoint::cpt.meanvar() %>%  # Identify change points
  autoplot()
# Detect jump in a data
strucchange::breakpoints(Nile ~ 1) %>%
  autoplot()
```


we note that there's actually three periods in the plot. there's varying course lengths: https://ade3.medium.com/bostons-evolution-1897-2018-cdd91aa79f95. We will check the relationship of the overall plot and the plot that's seperated but year. 

INCLUDE PLOT SEPERATED BY YEAR.



```{r}

# Kinda useless

install.packages("ggpmisc")
library(ggpmisc)
ggplot(marathon, as.numeric = FALSE) + geom_line() + 
  stat_peaks(colour = "red") +
  stat_peaks(geom = "text", colour = "red", 
             vjust = -0.5, x.label.fmt = "%Y") +
  stat_valleys(colour = "blue") +
  stat_valleys(geom = "text", colour = "blue", angle = 45,
               vjust = 1.5, hjust = 1,  x.label.fmt = "%Y")+
  ylim(-500, 7300)
```


```{r}
#USING THE POLYNOMIAL

abs(polyroot(c(-0.69))) #NO POINT SINCE WE HAVE ONE

```




```{r}
x=marathon
fit<-arima(x,order=c(0,1,1),include.mean=FALSE)
forecast<-predict(fit,n.ahead=50)
ts.plot(x,lty=3, main="MA(1) and prediction")
lines(x,lwd=2)
lines(forecast$pred,lwd=2,col="red")
lines(forecast$pred+forecast$se*1.96,lwd=2,col="red")
lines(forecast$pred-forecast$se*1.96,lwd=2,col="red")
```



```{r}
x=arima.sim(list(order=c(0,0,1), ma=0.7),200)
bt<-window(x,1,176)
fit<-arima(bt,order=c(0,1,1),include.mean=FALSE)
forecast<-predict(fit,n.ahead=24)
ts.plot(x,lty=3, main="MA(1) and prediction")
lines(bt,lwd=2)
lines(forecast$pred,lwd=2,col="red")
lines(forecast$pred+forecast$se*1.96,lwd=2,col="red")
lines(forecast$pred-forecast$se*1.96,lwd=2,col="red")
```




```{r}

#I DON'T THINK IT'S WORKING

lmarathon %>%
  forecast(h=36) %>%
  {cbind(actuals=.$x, forecast_mean=.$mean,
         lower_95=.$lower[,"95%"], upper_95=.$upper[,"95%"],
         lower_80=.$lower[,"80%"], upper_80=.$upper[,"80%"])} %>%
  dygraph() %>%
  dySeries("actuals", color = "black") %>%
  dySeries(c("lower_80", "forecast_mean", "upper_80"),
           label = "80%", color = "blue") %>%
  dySeries(c("lower_95", "forecast_mean", "upper_95"),
           label = "95%", color = "blue")
```

```{r}
pm <- forecast(fit, h=12)
plot(pm)
```

